version: '3.8'

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: wazuh-llm-backend
    ports:
      - "${BACKEND_PORT:-8000}:${BACKEND_PORT:-8000}"
    environment:
      # OpenAI
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o}
      - OPENAI_MAX_TOKENS=${OPENAI_MAX_TOKENS:-4096}
      - OPENAI_TIMEOUT=${OPENAI_TIMEOUT:-30}
      
      # MCP
      - MCP_SERVER_URL=${MCP_SERVER_URL:-http://host.docker.internal:3000}
      - MCP_CONNECTION_TIMEOUT=${MCP_CONNECTION_TIMEOUT:-10}
      - MCP_REQUEST_TIMEOUT=${MCP_REQUEST_TIMEOUT:-30}
      - MCP_MAX_RETRIES=${MCP_MAX_RETRIES:-3}
      
      # Backend
      - BACKEND_HOST=${BACKEND_HOST:-0.0.0.0}
      - BACKEND_PORT=${BACKEND_PORT:-8000}
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - DEBUG=${DEBUG:-false}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
      # Session
      - SESSION_MAX_MESSAGES=${SESSION_MAX_MESSAGES:-10}
      - SESSION_TIMEOUT_MINUTES=${SESSION_TIMEOUT_MINUTES:-60}
      - SESSION_CLEANUP_INTERVAL=${SESSION_CLEANUP_INTERVAL:-15}
      
      # LLM
      - MAX_TOOL_ITERATIONS=${MAX_TOOL_ITERATIONS:-5}
      - ENABLE_STREAMING=${ENABLE_STREAMING:-false}
      
      # Guardrails
      - STRICT_GUARDRAILS=${STRICT_GUARDRAILS:-true}
      
      # CORS
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3001,http://localhost:3000}
      
      # Performance
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS:-10}
      - RATE_LIMIT_PER_MINUTE=${RATE_LIMIT_PER_MINUTE:-30}
    
    extra_hosts:
      - "host.docker.internal:host-gateway"
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${BACKEND_PORT:-8000}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    
    restart: unless-stopped
    
    networks:
      - wazuh-llm-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - VITE_API_URL=${VITE_API_URL:-http://localhost:8000}
    container_name: wazuh-llm-frontend
    ports:
      - "${FRONTEND_PORT:-3001}:${FRONTEND_PORT:-3001}"
    environment:
      - VITE_API_URL=${VITE_API_URL:-http://localhost:8000}
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - wazuh-llm-network

networks:
  wazuh-llm-network:
    driver: bridge
